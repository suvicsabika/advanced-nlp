{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XhgzdIL6xvlX",
        "outputId": "7bae9d0a-f1ed-4cf1-df7e-21bc23553423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n",
            "  Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
            "Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, curated-tokenizers, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 spacy-curated-transformers-0.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 lxml spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "!pip install transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow\n",
        "!pip install requests beautifulsoup4 lxml spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install hf_xet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from functools import lru_cache\n",
        "import os\n",
        "import re\n",
        "from transformers import pipeline # For zero-shot, summarization, text2text-generation, and NER\n",
        "\n",
        "# --- Configuration ---\n",
        "SPACY_MODEL_TO_LOAD = \"en_core_web_trf\" # Using a consistent model name\n",
        "REQUEST_HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "OUTPUT_DIR = \"processed_articles\" # Directory for output files\n",
        "\n",
        "# --- 1. Load English spaCy model ---\n",
        "nlp_en = None\n",
        "try:\n",
        "    nlp_en = spacy.load(SPACY_MODEL_TO_LOAD, disable=[\"parser\", \"ner\"]) # spaCy NER is disabled as we will use HF NER\n",
        "    nlp_en.add_pipe(\"sentencizer\")\n",
        "    print(f\"English spaCy model ({SPACY_MODEL_TO_LOAD}) loaded successfully.\")\n",
        "except OSError:\n",
        "    print(f\"ERROR: English spaCy model ({SPACY_MODEL_TO_LOAD}) not installed. Please install it with the command:\")\n",
        "    print(f\"python -m spacy download {SPACY_MODEL_TO_LOAD}\")\n",
        "    print(\"The program will exit as the model is not installed.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load Hugging Face Pipelines ---\n",
        "zero_shot_classifier = None\n",
        "summarizer_pipeline = None\n",
        "title_generator_pipeline = None\n",
        "ner_pipeline = None # NEW: NER Pipeline\n",
        "\n",
        "try:\n",
        "    print(\"Loading Zero-shot classification pipeline...\")\n",
        "    zero_shot_classifier = pipeline(\"zero-shot-classification\",model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n",
        "    print(\"Zero-shot classification pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load the zero-shot classification pipeline: {e}\")\n",
        "    print(\"Categorization will not be available.\")\n",
        "\n",
        "try:\n",
        "    print(\"Loading Summarization pipeline...\")\n",
        "    summarizer_pipeline = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    print(\"Summarization pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load the summarization pipeline: {e}\")\n",
        "    print(\"Summarization will not be available.\")\n",
        "\n",
        "try:\n",
        "    print(\"Loading Text-to-Text Generation pipeline for titles...\")\n",
        "    title_generator_pipeline = pipeline(\"text2text-generation\", model=\"t5-large\")\n",
        "    print(\"Text-to-Text Generation pipeline (for titles using t5-large) loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load the Text-to-Text Generation pipeline (t5-large): {e}\")\n",
        "    print(\"Title generation will not be available.\")\n",
        "\n",
        "try:\n",
        "    print(\"Loading NER pipeline (dslim/bert-base-NER)...\") # NEW: Load NER pipeline\n",
        "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "    print(\"NER pipeline (dslim/bert-base-NER) loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to load the NER pipeline (dslim/bert-base-NER): {e}\")\n",
        "    print(\"Named Entity Recognition will not be available.\")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def fetch_html_content(url):\n",
        "    \"\"\"Fetches HTML content from the given URL.\"\"\"\n",
        "    try:\n",
        "        if \"news.google.com/read\" in url:\n",
        "            print(f\"Warning: The URL '{url}' appears to be a Google News redirector. \")\n",
        "            print(\"Attempting to fetch, but the content might be from the redirect page, not the final article.\")\n",
        "            print(\"For best results, try to use the direct article URL.\")\n",
        "\n",
        "        response = requests.get(url, headers=REQUEST_HEADERS, timeout=20, allow_redirects=True)\n",
        "        response.raise_for_status()\n",
        "        response.encoding = response.apparent_encoding if response.apparent_encoding else 'utf-8'\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error occurred while fetching URL '{url}': {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_html(html_string, main_content_selectors=None, selectors_to_exclude=None):\n",
        "    \"\"\"\n",
        "    Extracts clean text from an HTML string.\n",
        "    \"\"\"\n",
        "    if not html_string:\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(html_string, 'lxml')\n",
        "    for element in soup([\"script\", \"style\"]):\n",
        "        element.decompose()\n",
        "    content_scope = soup\n",
        "    if main_content_selectors:\n",
        "        for selector in main_content_selectors:\n",
        "            selected_block = soup.select_one(selector)\n",
        "            if selected_block:\n",
        "                content_scope = selected_block\n",
        "                break\n",
        "    if selectors_to_exclude and content_scope:\n",
        "        for selector in selectors_to_exclude:\n",
        "            for element_to_remove in content_scope.select(selector):\n",
        "                element_to_remove.decompose()\n",
        "    if content_scope == soup: # If no specific main content found, try to remove common boilerplate\n",
        "        common_boilerplate_tags = [\"header\", \"footer\", \"nav\", \"aside\", \"form\", \"figure\", \"figcaption\"] # Added figure/figcaption\n",
        "        for tag_name in common_boilerplate_tags:\n",
        "            for tag in content_scope.find_all(tag_name):\n",
        "                tag.decompose()\n",
        "    text = \"\"\n",
        "    if content_scope:\n",
        "        text = content_scope.get_text(separator=' ', strip=True)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# --- NLP Processing Functions ---\n",
        "@lru_cache(maxsize=128)\n",
        "def get_tokenized_text_spacy(text_to_process):\n",
        "    \"\"\"Tokenizes the text using spaCy.\"\"\"\n",
        "    if not nlp_en or not text_to_process:\n",
        "        return \"\"\n",
        "    doc = nlp_en(text_to_process)\n",
        "    tokens = [token.text for token in doc if not token.is_space]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_lemmatized_text_spacy(text_to_process):\n",
        "    \"\"\"Lemmatizes the English text.\"\"\"\n",
        "    if not nlp_en or not text_to_process:\n",
        "        return [], \"\"\n",
        "    doc = nlp_en(text_to_process)\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        if (not token.is_punct and\n",
        "            not token.is_space and\n",
        "            len(token.lemma_) > 0):\n",
        "            lemma_to_add = token.lemma_.lower()\n",
        "            if token.pos_ == \"PRON\" and token.lemma_ == \"-PRON-\": # spaCy uses -PRON- for pronouns\n",
        "                lemma_to_add = token.text.lower()\n",
        "            if lemma_to_add and (token.is_alpha or token.like_num): # Ensure lemma is alphanumeric or number-like\n",
        "                lemmas.append(lemma_to_add)\n",
        "    return lemmas, \" \".join(lemmas)\n",
        "\n",
        "# --- Categorization Function ---\n",
        "def categorize_text_zero_shot(text_to_categorize, categories, classifier_pipeline):\n",
        "    \"\"\"\n",
        "    Categorizes the text using a zero-shot method.\n",
        "    \"\"\"\n",
        "    if not classifier_pipeline:\n",
        "        print(\"Warning: Zero-shot classifier not loaded. Skipping categorization.\")\n",
        "        return None\n",
        "    if not text_to_categorize or not categories:\n",
        "        print(\"Warning: No text or categories provided for categorization.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        max_chars = 2048 # Model dependent, check model card for specific limits if issues arise\n",
        "        truncated_text = text_to_categorize[:max_chars]\n",
        "        result = classifier_pipeline(truncated_text, candidate_labels=categories, multi_label=False) # Assuming single label prediction\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during text categorization: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Summarization Function ---\n",
        "def generate_summary_abstractive(text_to_summarize, summarizer):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary of the text.\n",
        "    \"\"\"\n",
        "    if not summarizer:\n",
        "        print(\"Warning: Summarizer pipeline not loaded. Skipping summarization.\")\n",
        "        return None\n",
        "    if not text_to_summarize:\n",
        "        print(\"Warning: No text provided for summarization.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        max_input_chars_for_summary = 4000\n",
        "        input_text = text_to_summarize[:max_input_chars_for_summary]\n",
        "        summary = summarizer(input_text, max_length=150, min_length=40, do_sample=False)\n",
        "\n",
        "        if summary and isinstance(summary, list) and len(summary) > 0 and 'summary_text' in summary[0]:\n",
        "            return summary[0]['summary_text']\n",
        "        else:\n",
        "            print(\"Warning: Summarizer did not return the expected output format.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during text summarization: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Title Generation Function ---\n",
        "def generate_title_t5(text_to_get_title_from, title_generator, max_input_chars=2048):\n",
        "    \"\"\"\n",
        "    Generates a title for the text using a T5 model.\n",
        "    \"\"\"\n",
        "    if not title_generator:\n",
        "        print(\"Warning: Title generator pipeline not loaded. Skipping title generation.\")\n",
        "        return None\n",
        "    if not text_to_get_title_from:\n",
        "        print(\"Warning: No text provided for title generation.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        input_text = \"generate headline: \" + text_to_get_title_from[:max_input_chars]\n",
        "        title_result = title_generator(input_text, max_length=30, min_length=5, num_beams=4, early_stopping=True)\n",
        "\n",
        "        if title_result and isinstance(title_result, list) and len(title_result) > 0 and 'generated_text' in title_result[0]:\n",
        "            return title_result[0]['generated_text']\n",
        "        else:\n",
        "            print(\"Warning: Title generator did not return the expected output format.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during title generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- NER Post-processing Helper Function (NEW) ---\n",
        "def post_process_ner_entities(raw_entities_list, score_threshold=0.70):\n",
        "    \"\"\"\n",
        "    Post-processes a list of raw NER entities.\n",
        "    - Filters by confidence score.\n",
        "    - Attempts to merge fragmented entities (basic).\n",
        "    - Filters known false positives.\n",
        "    - Collects unique entities, keeping the one with the highest score.\n",
        "    - Sorts entities by score.\n",
        "    \"\"\"\n",
        "    if not raw_entities_list:\n",
        "        return []\n",
        "\n",
        "    # 1. Filter by score\n",
        "    processed_entities = [ent for ent in raw_entities_list if ent['score'] >= score_threshold]\n",
        "\n",
        "    # 2. Attempt to merge fragmented entities (basic approach)\n",
        "    # This is a simplified merge and might need refinement for complex cases.\n",
        "    if processed_entities:\n",
        "        merged_attempt_entities = []\n",
        "        i = 0\n",
        "        while i < len(processed_entities):\n",
        "            current_entity = processed_entities[i]\n",
        "            # Try to merge if current starts with \"##\" and there's a previous entity of the same type\n",
        "            # and the previous entity does not end with a space (suggesting it might be an incomplete token)\n",
        "            if current_entity['text'].startswith(\"##\") and merged_attempt_entities:\n",
        "                prev_entity_dict = merged_attempt_entities[-1]\n",
        "                if prev_entity_dict['type'] == current_entity['type'] and \\\n",
        "                   not prev_entity_dict['text'].endswith(\" \"):\n",
        "                    original_prev_score = prev_entity_dict['score']\n",
        "                    prev_entity_dict['text'] += current_entity['text'].replace(\"##\", \"\")\n",
        "                    prev_entity_dict['score'] = (original_prev_score + current_entity['score']) / 2 # Average score\n",
        "                    i += 1\n",
        "                    continue\n",
        "            merged_attempt_entities.append(dict(current_entity)) # Use a copy\n",
        "            i += 1\n",
        "        processed_entities = merged_attempt_entities\n",
        "\n",
        "    # 3. Filter known false positives or very short/generic entities\n",
        "    #    (Customize this list based on observed errors)\n",
        "    known_false_positives_text_type = {\n",
        "        (\"Piano\", \"PER\"),\n",
        "        (\"Man\", \"PER\"), # If \"Man\" alone as PER is usually wrong\n",
        "        (\"In\", \"ORG\"),\n",
        "        (\"Pop\", \"PER\"), # If \"Pop\" alone as PER is usually wrong\n",
        "    }\n",
        "    entities_after_fp_filter = []\n",
        "    for ent in processed_entities:\n",
        "        # Check against known false positives\n",
        "        if (ent['text'], ent['type']) in known_false_positives_text_type:\n",
        "            continue\n",
        "        # Filter out very short, non-alphanumeric entities or common words misclassified\n",
        "        # (this is a heuristic and can be adjusted)\n",
        "        text_lower = ent['text'].lower()\n",
        "        if len(text_lower) < 2 and not text_lower.isalnum(): # e.g. single punctuation\n",
        "            continue\n",
        "        if len(text_lower) < 3 and ent['type'] in ['ORG', 'PER'] and not any(char.isupper() for char in ent['text']):\n",
        "            # e.g. \"an\" as ORG, \"he\" as PER, unless it has caps (which might indicate actual name)\n",
        "            if text_lower in [\"he\", \"she\", \"it\", \"we\", \"us\", \"an\", \"in\", \"on\", \"at\", \"of\", \"to\", \"is\", \"a\"]: # Common stopwords\n",
        "                 continue\n",
        "        entities_after_fp_filter.append(ent)\n",
        "    processed_entities = entities_after_fp_filter\n",
        "\n",
        "    # 4. Collect unique entities (text, type), keeping the one with the highest score\n",
        "    unique_entities_dict = {}\n",
        "    for ent in processed_entities:\n",
        "        key = (ent['text'].strip(), ent['type']) # Use stripped text for key\n",
        "        if key not in unique_entities_dict or ent['score'] > unique_entities_dict[key]['score']:\n",
        "            # Ensure text is stripped before storing if it wasn't already\n",
        "            ent_copy = dict(ent)\n",
        "            ent_copy['text'] = ent['text'].strip()\n",
        "            unique_entities_dict[key] = ent_copy\n",
        "    processed_entities = list(unique_entities_dict.values())\n",
        "\n",
        "    # 5. Sort entities by score (descending) for consistent output\n",
        "    processed_entities.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return processed_entities\n",
        "\n",
        "# --- NER Function (MODIFIED to include post-processing) ---\n",
        "def extract_named_entities_bert(text_to_process, ner_model_pipeline, max_chars=4000):\n",
        "    \"\"\"\n",
        "    Extracts and post-processes named entities from text using a Hugging Face NER pipeline.\n",
        "    \"\"\"\n",
        "    if not ner_model_pipeline:\n",
        "        print(\"Warning: NER pipeline not loaded. Skipping entity extraction.\")\n",
        "        return None, \"\"\n",
        "    if not text_to_process:\n",
        "        print(\"Warning: No text provided for entity extraction.\")\n",
        "        return [], \"\" # Return empty list and string if no text\n",
        "\n",
        "    try:\n",
        "        truncated_text = text_to_process[:max_chars]\n",
        "        entities_from_pipeline = ner_model_pipeline(truncated_text)\n",
        "\n",
        "        raw_formatted_entities = []\n",
        "        if entities_from_pipeline:\n",
        "            for entity in entities_from_pipeline:\n",
        "                raw_formatted_entities.append({\n",
        "                    \"text\": entity['word'],\n",
        "                    \"type\": entity['entity_group'],\n",
        "                    \"score\": float(entity['score'])\n",
        "                })\n",
        "\n",
        "        # Call post-processing function\n",
        "        if raw_formatted_entities:\n",
        "            # You can adjust the score_threshold here if needed for different contexts\n",
        "            processed_entities_list = post_process_ner_entities(raw_formatted_entities, score_threshold=0.65)\n",
        "        else:\n",
        "            processed_entities_list = []\n",
        "\n",
        "        if processed_entities_list:\n",
        "            entity_string_parts = [f\"Entity: {e['text']}, Type: {e['type']} (Score: {e['score']:.4f})\" for e in processed_entities_list]\n",
        "            return processed_entities_list, \"\\n\".join(entity_string_parts)\n",
        "        else:\n",
        "            return [], \"\" # Return empty if no entities after processing or initially\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during Named Entity Recognition: {e}\")\n",
        "        return None, \"\" # Indicate error to the caller\n",
        "\n",
        "# --- File Handling Functions ---\n",
        "def save_text_to_file(text_content, filepath):\n",
        "    \"\"\"Saves text content to a file.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text_content)\n",
        "        print(f\"   File saved successfully: {filepath}\")\n",
        "    except IOError as e:\n",
        "        print(f\"   Error writing file ({filepath}): {e}\")\n",
        "\n",
        "def generate_safe_filename(url_or_title, suffix=\"\"):\n",
        "    \"\"\"Generates a safe filename.\"\"\"\n",
        "    if not url_or_title:\n",
        "        base_name = f\"article_{suffix}\"\n",
        "    else:\n",
        "        name_part = url_or_title.split('/')[-1] if '/' in url_or_title else url_or_title\n",
        "        name_part = name_part.split('?')[0] # Remove query parameters\n",
        "        safe_name = re.sub(r'[^a-zA-Z0-9_-]+', '_', name_part).strip('_')\n",
        "        safe_name = safe_name[:50] # Limit length\n",
        "        base_name = f\"{safe_name}_{suffix}\" if safe_name else f\"article_{suffix}\"\n",
        "    return os.path.join(OUTPUT_DIR, f\"{base_name}.txt\")\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_article_url(article_url, main_content_selectors=None, selectors_to_exclude=None):\n",
        "    \"\"\"\n",
        "    Executes the full processing pipeline for a given URL.\n",
        "    \"\"\"\n",
        "    if not nlp_en: # spaCy for tokenization/lemmatization\n",
        "        print(\"spaCy model not available. Some NLP processing (tokenization, lemmatization) cannot be started.\")\n",
        "\n",
        "    print(f\"Processing started: {article_url}\")\n",
        "\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        try:\n",
        "            os.makedirs(OUTPUT_DIR)\n",
        "            print(f\"Output directory created: {OUTPUT_DIR}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating output directory ({OUTPUT_DIR}): {e}\")\n",
        "            return\n",
        "\n",
        "    print(f\"\\n1. Fetching HTML content...\")\n",
        "    html_code = fetch_html_content(article_url)\n",
        "    if not html_code:\n",
        "        print(\"   Failed to download HTML content. Processing stopped.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n2. Extracting text from HTML...\")\n",
        "    raw_cleaned_text = extract_text_from_html(html_code,\n",
        "                                               main_content_selectors=main_content_selectors,\n",
        "                                               selectors_to_exclude=selectors_to_exclude)\n",
        "    if not raw_cleaned_text:\n",
        "        print(\"   Failed to extract text from HTML. Processing stopped.\")\n",
        "        return\n",
        "    print(f\"   Raw text extracted (first 200 characters): '{raw_cleaned_text[:200]}...'\")\n",
        "    raw_text_filepath = generate_safe_filename(article_url, \"raw_specific\")\n",
        "    save_text_to_file(raw_cleaned_text, raw_text_filepath)\n",
        "\n",
        "    if nlp_en:\n",
        "        print(\"\\n3. Tokenizing text with spaCy...\")\n",
        "        tokenized_text = get_tokenized_text_spacy(raw_cleaned_text)\n",
        "        if tokenized_text:\n",
        "            tokenized_text_filepath = generate_safe_filename(article_url, \"tokenized_specific\")\n",
        "            save_text_to_file(tokenized_text, tokenized_text_filepath)\n",
        "        else:\n",
        "            print(\"   Failed to tokenize text.\")\n",
        "\n",
        "        print(\"\\n4. Lemmatizing text with spaCy...\")\n",
        "        _, lemmatized_text_string = get_lemmatized_text_spacy(raw_cleaned_text) # We need the string output here\n",
        "        if lemmatized_text_string:\n",
        "            lemmatized_text_filepath = generate_safe_filename(article_url, \"lemmatized_specific\")\n",
        "            save_text_to_file(lemmatized_text_string, lemmatized_text_filepath)\n",
        "        else:\n",
        "            print(\"   Failed to lemmatize text.\")\n",
        "    else:\n",
        "        print(\"\\nWarning: spaCy model not loaded, skipping tokenization and lemmatization.\")\n",
        "\n",
        "    print(\"\\n5. Categorizing text (zero-shot)...\")\n",
        "    if raw_cleaned_text and zero_shot_classifier:\n",
        "        candidate_article_categories = [\n",
        "            \"sport\", \"football\", \"politics\", \"business\", \"finance\", \"technology\", \"science\",\n",
        "            \"health\", \"medicine\", \"education\",  \"music\", \"movie\",\"world news\", \"culture\", \"art\", \"travel\", \"food\", \"lifestyle\", \"environment\",\n",
        "            \"social issues\", \"mental health\", \"entertainment\"\n",
        "        ]\n",
        "        category_results = categorize_text_zero_shot(raw_cleaned_text, candidate_article_categories, zero_shot_classifier)\n",
        "\n",
        "        if category_results and category_results['labels'] and category_results['scores']:\n",
        "            predicted_label = category_results['labels'][0]\n",
        "            predicted_score = category_results['scores'][0]\n",
        "            print(f\"   Predicted category: {predicted_label} (Score: {predicted_score:.4f})\")\n",
        "        else:\n",
        "            print(\"   Could not determine a clear category or an error occurred.\")\n",
        "    elif not zero_shot_classifier:\n",
        "        print(\"   Skipping categorization because the zero-shot classifier is not loaded.\")\n",
        "    else:\n",
        "        print(\"   No text available for categorization.\")\n",
        "\n",
        "    print(\"\\n6. Generating text summary...\")\n",
        "    if raw_cleaned_text and summarizer_pipeline:\n",
        "        article_summary = generate_summary_abstractive(raw_cleaned_text, summarizer_pipeline)\n",
        "        if article_summary:\n",
        "            print(f\"   Generated summary: {article_summary}\")\n",
        "            summary_filepath = generate_safe_filename(article_url, \"summary_specific\")\n",
        "            save_text_to_file(article_summary, summary_filepath)\n",
        "        else:\n",
        "            print(\"   Failed to generate summary.\")\n",
        "    elif not summarizer_pipeline:\n",
        "        print(\"   Skipping summarization because the summarizer pipeline is not loaded.\")\n",
        "    else:\n",
        "        print(\"   No text available for summarization.\")\n",
        "\n",
        "    print(\"\\n7. Generating title...\")\n",
        "    if raw_cleaned_text and title_generator_pipeline:\n",
        "        generated_raw_title = generate_title_t5(raw_cleaned_text, title_generator_pipeline)\n",
        "        processed_title = \"\"\n",
        "        if generated_raw_title:\n",
        "            generated_raw_title = generated_raw_title.strip()\n",
        "            first_period_index = generated_raw_title.find('.')\n",
        "            if first_period_index != -1:\n",
        "                processed_title = generated_raw_title[:first_period_index + 1]\n",
        "            else:\n",
        "                processed_title = generated_raw_title\n",
        "            if processed_title:\n",
        "                processed_title = processed_title[0].upper() + processed_title[1:]\n",
        "            print(f\"   Generated title: {processed_title}\")\n",
        "            title_filepath = generate_safe_filename(article_url, \"title_specific\")\n",
        "            save_text_to_file(processed_title, title_filepath)\n",
        "        else:\n",
        "            print(\"   Failed to generate title (empty result from generator).\")\n",
        "    elif not title_generator_pipeline:\n",
        "        print(\"   Skipping title generation because the title generator pipeline is not loaded.\")\n",
        "    else:\n",
        "        print(\"   No text available for title generation.\")\n",
        "\n",
        "    # --- 8. Extracting Named Entities (BERT with Post-processing) ---\n",
        "    print(\"\\n8. Extracting Named Entities (BERT with Post-processing)...\")\n",
        "    if raw_cleaned_text and ner_pipeline:\n",
        "        # extract_named_entities_bert now returns post-processed entities\n",
        "        processed_entities_list, processed_entities_string = extract_named_entities_bert(raw_cleaned_text, ner_pipeline)\n",
        "\n",
        "        if processed_entities_list is not None: # Check if NER processing encountered an error\n",
        "            if processed_entities_list: # Check if the list is not empty after post-processing\n",
        "                print(f\"   Extracted {len(processed_entities_list)} entities after post-processing.\")\n",
        "                # Print first few (e.g., 10) post-processed entities\n",
        "                for i, entity in enumerate(processed_entities_list[:10]):\n",
        "                    print(f\"     - {entity['text']} ({entity['type']}, Score: {entity['score']:.3f})\")\n",
        "                if len(processed_entities_list) > 10:\n",
        "                    print(f\"     ... and {len(processed_entities_list) - 10} more.\")\n",
        "\n",
        "                # Save the post-processed entities string to a file\n",
        "                entities_filepath = generate_safe_filename(article_url, \"entities_processed_specific\") # Changed suffix\n",
        "                save_text_to_file(processed_entities_string, entities_filepath)\n",
        "            else:\n",
        "                print(\"   No entities found in the text after post-processing.\")\n",
        "        else:\n",
        "            # This case handles if extract_named_entities_bert returned None (due to an internal error)\n",
        "            print(\"   Failed to extract entities or an error occurred during NER processing.\")\n",
        "    elif not ner_pipeline:\n",
        "        print(\"   Skipping Named Entity Recognition because the NER pipeline is not loaded.\")\n",
        "    else:\n",
        "        print(\"   No text available for Named Entity Recognition.\")\n",
        "\n",
        "    print(f\"\\nProcessing finished for: {article_url}\")\n",
        "\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if essential models are loaded. The script can still run partially if some are missing.\n",
        "    if not nlp_en:\n",
        "        print(\"Warning: The spaCy NLP model (for tokenization/lemmatization) did not load. Some functionalities will be skipped.\")\n",
        "    if not zero_shot_classifier:\n",
        "        print(\"Warning: Zero-shot classification pipeline did not load. Categorization will be skipped.\")\n",
        "    if not summarizer_pipeline:\n",
        "        print(\"Warning: Summarization pipeline did not load. Summarization will be skipped.\")\n",
        "    if not title_generator_pipeline:\n",
        "        print(\"Warning: Title generation pipeline did not load. Title generation will be skipped.\")\n",
        "    if not ner_pipeline: # NEW: Check for NER pipeline\n",
        "        print(\"Warning: NER pipeline did not load. Named Entity Recognition will be skipped.\")\n",
        "\n",
        "\n",
        "    url_to_process = \"https://www.goal.com/en-sa/lists/winners-losers-2024-25-premier-league-season/blt2a1709cc33840e04\"\n",
        "    # Example with more prominent entities:\n",
        "    # url_to_process = \"https://www.reuters.com/world/europe/putin-xi-map-out-new-era-press-conference-after-kremlin-talks-2023-03-21/\"\n",
        "\n",
        "    main_selectors_for_billboard = None\n",
        "    exclude_selectors_for_billboard = None\n",
        "\n",
        "    process_article_url(url_to_process,main_selectors_for_billboard,exclude_selectors_for_billboard)\n",
        "\n",
        "    # Example of processing another URL with potentially different selectors\n",
        "    # url_reuters = \"https://www.reuters.com/technology/musk-says-xaitests-grok-chatbot-with-more-users-after-political-bias-concerns-2024-03-15/\"\n",
        "    # main_selectors_reuters = ['article[data-testid=\"ArticlePage-article-body\"]']\n",
        "    # exclude_selectors_reuters = ['div[data-testid=\"Paywall-Container\"]', 'div[data-testid=\"AdditionalCoverage-Container\"]']\n",
        "    # process_article_url(url_reuters, main_selectors_reuters, exclude_selectors_reuters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uisKQUUZyuDS",
        "outputId": "e6399da8-aab9-40e6-9231-e07e485ac27e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English spaCy model (en_core_web_trf) loaded successfully.\n",
            "Loading Zero-shot classification pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot classification pipeline loaded successfully.\n",
            "Loading Summarization pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization pipeline loaded successfully.\n",
            "Loading Text-to-Text Generation pipeline for titles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text-to-Text Generation pipeline (for titles using t5-large) loaded successfully.\n",
            "Loading NER pipeline (dslim/bert-base-NER)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER pipeline (dslim/bert-base-NER) loaded successfully.\n",
            "Processing started: https://www.goal.com/en-sa/lists/winners-losers-2024-25-premier-league-season/blt2a1709cc33840e04\n",
            "\n",
            "1. Fetching HTML content...\n",
            "\n",
            "2. Extracting text from HTML...\n",
            "   Raw text extracted (first 200 characters): 'Arne Slot, Sir Jim Ratcliffe and the biggest winners and losers of the 2024-25 Premier League season | Goal.com English Saudi Arabia Getty Images Mark Doyle Winners & Losers Premier League Liverpool A...'\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_raw_specific.txt\n",
            "\n",
            "3. Tokenizing text with spaCy...\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_tokenized_specific.txt\n",
            "\n",
            "4. Lemmatizing text with spaCy...\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_lemmatized_specific.txt\n",
            "\n",
            "5. Categorizing text (zero-shot)...\n",
            "   Predicted category: football (Score: 0.4574)\n",
            "\n",
            "6. Generating text summary...\n",
            "   Generated summary:  Liverpool won the Premier League title for the first time in 20 years . Manchester City, Manchester United, Newcastle, Tottenham, Southampton, Tottenham and Tottenham all endured historically-bad league campaigns . GOAL runs through the big winners and losers of the 2024-25 Premier League season .\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_summary_specific.txt\n",
            "\n",
            "7. Generating title...\n",
            "   Generated title: :: Arne Slot, Sir Jim Ratcliffe and the biggest winners and losers of the 2024-25 Premier League season\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_title_specific.txt\n",
            "\n",
            "8. Extracting Named Entities (BERT with Post-processing)...\n",
            "   Extracted 32 entities after post-processing.\n",
            "     - Liverpool (ORG, Score: 0.999)\n",
            "     - Southampton (ORG, Score: 0.999)\n",
            "     - Manchester United (ORG, Score: 0.999)\n",
            "     - Tottenham (ORG, Score: 0.999)\n",
            "     - Ipswich Town (ORG, Score: 0.999)\n",
            "     - Leicester City (ORG, Score: 0.998)\n",
            "     - Champions League (MISC, Score: 0.998)\n",
            "     - Brentford Brighton (ORG, Score: 0.998)\n",
            "     - Liverpool Arsenal (ORG, Score: 0.997)\n",
            "     - Amor (PER, Score: 0.997)\n",
            "     ... and 22 more.\n",
            "   File saved successfully: processed_articles/blt2a1709cc33840e04_entities_processed_specific.txt\n",
            "\n",
            "Processing finished for: https://www.goal.com/en-sa/lists/winners-losers-2024-25-premier-league-season/blt2a1709cc33840e04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6GMYzpgyQ_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKhlAs2KFKIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}