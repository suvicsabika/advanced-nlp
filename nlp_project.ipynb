{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PGGLtATAVbRQ",
      "metadata": {
        "id": "PGGLtATAVbRQ"
      },
      "source": [
        "# Weboldal Tartalom Feldolgozása és Kategorizálása"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WXoUHaAmVbRU",
      "metadata": {
        "id": "WXoUHaAmVbRU"
      },
      "source": [
        "## 0. Könyvtárak Importálása és Szükséges Csomagok Telepítése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "BTEyAG_IVbRV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTEyAG_IVbRV",
        "outputId": "c2305aea-0cef-40db-c648-f7590a8ec384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: unicodedata2 in /usr/local/lib/python3.11/dist-packages (16.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: huspacy in /usr/local/lib/python3.11/dist-packages (0.12.1)\n",
            "Requirement already satisfied: packaging<22.0,>=21.3 in /usr/local/lib/python3.11/dist-packages (from huspacy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from packaging<22.0,>=21.3->huspacy) (3.2.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from packaging>=20.0->transformers) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "# Szükséges csomagok telepítése (Colab/Jupyter környezetben)\n",
        "!pip install requests beautifulsoup4 pandas lxml unicodedata2\n",
        "!pip install huspacy\n",
        "!pip install transformers sentencepiece torch\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "urc8O6vLVbRW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urc8O6vLVbRW",
        "outputId": "8d1d6851-6ee4-4361-a3bd-2b1cee3613e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/usr/bin/python3', '-m', 'pip', 'install', 'hu_core_news_lg @ https://huggingface.co/huspacy/hu_core_news_lg/resolve/v3.8.0/hu_core_news_lg-any-py3-none-any.whl']\n"
          ]
        }
      ],
      "source": [
        "import requests                     # HTTP kérésekhez (weboldal letöltése)\n",
        "from bs4 import BeautifulSoup        # HTML parse-oláshoz\n",
        "import pandas as pd                 # Adatkezeléshez (DataFrame)\n",
        "import re                           # Reguláris kifejezésekhez\n",
        "import unicodedata                  # Unicode normalizáláshoz\n",
        "from functools import lru_cache\n",
        "\n",
        "import huspacy\n",
        "#huspacy.download() # Ha lokálisan futtatod és még nincs letöltve a modell, vagy ha a lentebbi try-except nem működik\n",
        "\n",
        "import spacy                        # NLP feladatokhoz (lemmatizálás)\n",
        "from sentence_transformers import SentenceTransformer # Beágyazások generálásához\n",
        "import torch                        # PyTorch (gyakran a transformers könyvtárak függősége)\n",
        "from transformers import pipeline   # Szövegkategorizáláshoz\n",
        "\n",
        "# Figyelmeztetések kikapcsolása (opcionális)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t2Ac9sZmVbRX",
      "metadata": {
        "id": "t2Ac9sZmVbRX"
      },
      "source": [
        "### Globális Beállítások és Modellek Betöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "agDf2rXIVbRY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agDf2rXIVbRY",
        "outputId": "33f57ea2-ba4b-4368-80ff-52eae0d97cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Magyar spaCy modell (huspacy alapú) sikeresen betöltve.\n"
          ]
        }
      ],
      "source": [
        "# Magyar spaCy modell betöltése huspacy segítségével\n",
        "nlp_hu = None\n",
        "try:\n",
        "    nlp_hu = huspacy.load()\n",
        "    print(\"Magyar spaCy modell (huspacy alapú) sikeresen betöltve.\")\n",
        "except OSError:\n",
        "    print(\"HIBA: A magyar spaCy modell (pl. hu_core_news_lg) nincs telepítve vagy nem tölthető be.\")\n",
        "    print(\"Próbáld meg futtatni a 'huspacy.download()' parancsot egy külön cellában, majd indítsd újra a futási környezetet.\")\n",
        "    print(\"Google Colab-ban futtasd: !python -m spacy download hu_core_news_lg, majd Runtime -> Restart session.\")\n",
        "\n",
        "# A cél URL\n",
        "TARGET_URL = \"https://www.aiwc.ca/blog/duckweed-a-superfood-from-the-wetlands/\"\n",
        "\n",
        "# Gyorsítótárazott lemmatizálás\n",
        "@lru_cache(maxsize=1024)\n",
        "def lemmatize_text_cached(text):\n",
        "    # Ezt a függvényt a kód nem használja közvetlenül, de jó mintaként szolgálhat\n",
        "    # A tényleges lemmatizálás a 'lemmatize_text_with_spacy' vagy 'lemmatize_texts_batch' függvényekben történik.\n",
        "    if nlp_hu:\n",
        "        doc = nlp_hu(str(text))\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "        return \" \".join(lemmas)\n",
        "    return str(text) # Fallback, ha nincs nlp_hu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65PYxHlXVbRZ",
      "metadata": {
        "id": "65PYxHlXVbRZ"
      },
      "source": [
        "## 1. HTML Tartalom Letöltése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Ug-xfM2LVbRZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug-xfM2LVbRZ",
        "outputId": "7d7be6e5-0591-4de3-a9f3-3fe98db2f48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 1. HTML Tartalom Letöltése: https://www.aiwc.ca/blog/duckweed-a-superfood-from-the-wetlands/ ---\n",
            "Az oldal sikeresen letöltve (hossz: 295471 karakter).\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- 1. HTML Tartalom Letöltése: {TARGET_URL} ---\")\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "html_content_article = None\n",
        "try:\n",
        "    response = requests.get(TARGET_URL, headers=headers, timeout=15)\n",
        "    response.raise_for_status()\n",
        "    html_content_article = response.text\n",
        "    print(f\"Az oldal sikeresen letöltve (hossz: {len(html_content_article)} karakter).\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Hiba történt az oldal letöltése közben: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "js_JP1JxVbRa",
      "metadata": {
        "id": "js_JP1JxVbRa"
      },
      "source": [
        "## 2. Fő Cikkszöveg Kinyerése a HTML-ből"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "twUAO7QOVbRb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twUAO7QOVbRb",
        "outputId": "23a24c07-e0b7-42bf-d858-fc4f3f1bdf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2. Fő Cikkszöveg Kinyerése ---\n",
            "Általános '<article>' tag alapján történő kinyerés.\n",
            "Talált cikktartalom 1 bekezdéssel.\n",
            "Kinyert nyers szöveg hossza: 136 karakter.\n",
            "Kinyert szöveg eleje (max 500 karakter):\n",
            "By Natalie Galan In the heart of Alberta, the bison, both plains and wood bison, are more than just iconic symbols of the Canadian West.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- 2. Fő Cikkszöveg Kinyerése ---\")\n",
        "extracted_article_text_parts = []\n",
        "if html_content_article:\n",
        "    soup = BeautifulSoup(html_content_article, \"lxml\")\n",
        "\n",
        "    article_body_specific = soup.find('div', class_='post-content')\n",
        "    if article_body_specific:\n",
        "        print(\"Specifikus 'post-content' div alapján történő kinyerés.\")\n",
        "        paragraphs = article_body_specific.find_all('p')\n",
        "    elif soup.find('article'):\n",
        "        print(\"Általános '<article>' tag alapján történő kinyerés.\")\n",
        "        article_body = soup.find('article')\n",
        "        paragraphs = article_body.find_all('p')\n",
        "    elif soup.find('div', role='article'):\n",
        "        print(\"Általános 'div[role=\\\"article\\\"]' tag alapján történő kinyerés.\")\n",
        "        article_body = soup.find('div', role='article')\n",
        "        paragraphs = article_body.find_all('p')\n",
        "    else:\n",
        "        print(\"Figyelem: Nem találtuk a szokásos cikk konténereket. Próbálkozás az összes <p> taggel a body-ból.\")\n",
        "        paragraphs = soup.find_all('p') if soup.body else []\n",
        "\n",
        "    if paragraphs:\n",
        "        print(f\"Talált cikktartalom {len(paragraphs)} bekezdéssel.\")\n",
        "        relevant_paragraphs = [p.get_text(separator=\" \", strip=True) for p in paragraphs]\n",
        "        extracted_article_text_parts.extend(relevant_paragraphs)\n",
        "    else:\n",
        "        print(\"Nem sikerült bekezdéseket (<p>) kinyerni a fő tartalomból.\")\n",
        "\n",
        "    full_extracted_text = \" \".join(filter(None, extracted_article_text_parts))\n",
        "\n",
        "    if not full_extracted_text.strip() and html_content_article:\n",
        "        print(\"Figyelem: A célzott kinyerés nem adott eredményt, próbálkozás a teljes body szövegével.\")\n",
        "        full_extracted_text = soup.body.get_text(separator=\" \", strip=True) if soup.body else \"Nem sikerült szöveget kinyerni.\"\n",
        "else:\n",
        "    full_extracted_text = \"Az oldal HTML tartalma nem érhető el.\"\n",
        "\n",
        "print(f\"Kinyert nyers szöveg hossza: {len(full_extracted_text)} karakter.\")\n",
        "print(\"Kinyert szöveg eleje (max 500 karakter):\")\n",
        "print(full_extracted_text[:500] + \"...\" if len(full_extracted_text) > 500 else full_extracted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1v5FVUBkVbRc",
      "metadata": {
        "id": "1v5FVUBkVbRc"
      },
      "source": [
        "## 3. Szöveg Tisztítása és Előfeldolgozása"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "WbCNhTcAVbRc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbCNhTcAVbRc",
        "outputId": "d2fd36d0-ad0f-4748-a659-5fc52fcea382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 3. Szöveg Tisztítása és Előfeldolgozása ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- 3. Szöveg Tisztítása és Előfeldolgozása ---\")\n",
        "\n",
        "# Adat DataFrame-be helyezése\n",
        "df_article_processed = pd.DataFrame({'id': [TARGET_URL], 'raw_extracted_text': [full_extracted_text]})\n",
        "\n",
        "# --- Segédfüggvények a tisztításhoz és feldolgozáshoz ---\n",
        "def clean_special_chars_and_normalize(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = unicodedata.normalize(\"NFKC\", text) # Unicode normalizálás\n",
        "    text = re.sub(r\"\\[\\d+\\]\", \"\", text) # [1], [23] stílusú hivatkozások eltávolítása\n",
        "    text = re.sub(r\"\\[\\d+-\\d+\\]\", \"\", text) # [1-5] stílusú hivatkozások eltávolítása\n",
        "    text = re.sub(r\"http\\S+\", \"\", text) # URL-ek eltávolítása\n",
        "    text = re.sub(r\"[^a-zA-Z0-9áéíóöőúüűÁÉÍÓÖŐÚÜŰ\\s.,!?:;\\-]\", \" \", text)\n",
        "    text = \" \".join(text.split()) # Extra szóközök eltávolítása\n",
        "    return text.strip()\n",
        "\n",
        "def lemmatize_texts_batch(texts, nlp_model):\n",
        "    if not nlp_model:\n",
        "        return texts # Visszaadjuk az eredeti szövegeket, ha nincs modell\n",
        "    # A huspacy.load() által visszaadott objektum maga a pipeline\n",
        "    docs = nlp_model.pipe(texts, batch_size=16, disable=[\"parser\", \"ner\"]) # Gyorsabb NER és parser nélkül\n",
        "    lemmatized_texts = []\n",
        "    for doc in docs:\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "        lemmatized_texts.append(\" \".join(lemmas))\n",
        "    return lemmatized_texts\n",
        "\n",
        "def split_text_to_chunks(text, max_chunk_tokens=500, tokenizer=None):\n",
        "    if not tokenizer:\n",
        "        raise ValueError(\"A 'tokenizer' paraméter megadása kötelező a split_text_to_chunks függvényhez.\")\n",
        "\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text) # Mondatbontás\n",
        "    chunks = []\n",
        "    current_chunk_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if not sentence.strip():\n",
        "            continue\n",
        "        temp_chunk_text = \" \".join(current_chunk_sentences + [sentence])\n",
        "        n_tokens = len(tokenizer.encode(temp_chunk_text))\n",
        "\n",
        "        if n_tokens <= max_chunk_tokens:\n",
        "            current_chunk_sentences.append(sentence)\n",
        "        else:\n",
        "            if current_chunk_sentences:\n",
        "                chunks.append(\" \".join(current_chunk_sentences).strip())\n",
        "\n",
        "            if len(tokenizer.encode(sentence)) <= max_chunk_tokens:\n",
        "                current_chunk_sentences = [sentence]\n",
        "            else: # Ha egyetlen mondat is túl hosszú\n",
        "                # print(f\"Figyelem: Egyetlen mondat ({len(tokenizer.encode(sentence))} token) hosszabb, mint max_chunk_tokens ({max_chunk_tokens}). Durva vágás lehet szükséges.\")\n",
        "                # A SentenceTransformer encode levágja, ha a mondat maga hosszabb a modell max_seq_length-jénél\n",
        "                chunks.append(sentence.strip())\n",
        "                current_chunk_sentences = []\n",
        "\n",
        "    if current_chunk_sentences:\n",
        "        chunks.append(\" \".join(current_chunk_sentences).strip())\n",
        "\n",
        "    chunks = [ch for ch in chunks if ch]\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_average_embedding(text, model, max_chunk_tokens=500): # model itt az sbert_model\n",
        "    if not text or not text.strip():\n",
        "        print(\"Üres szöveg a beágyazáshoz.\")\n",
        "        try:\n",
        "            dim = model.get_sentence_embedding_dimension()\n",
        "            return torch.zeros(dim).numpy()\n",
        "        except: return None\n",
        "\n",
        "    chunks = split_text_to_chunks(text, max_chunk_tokens=max_chunk_tokens, tokenizer=model.tokenizer)\n",
        "    if not chunks:\n",
        "        print(\"Figyelem: Nem sikerült darabokra (chunk) bontani a szöveget a beágyazáshoz.\")\n",
        "        try:\n",
        "            dim = model.get_sentence_embedding_dimension()\n",
        "            return torch.zeros(dim).numpy()\n",
        "        except:\n",
        "             return None\n",
        "\n",
        "    embeddings = model.encode(chunks, show_progress_bar=False)\n",
        "    average_embedding = sum(torch.tensor(e) for e in embeddings) / len(embeddings)\n",
        "    return average_embedding.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "GlUOtb41VbRd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlUOtb41VbRd",
        "outputId": "e57791be-b239-4737-aadc-a49ef9c679eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatizálás folyamatban...\n",
            "Lemmatizálás befejezve.\n",
            "Tisztított szöveg eleje:\n",
            "By Natalie Galan In the heart of Alberta, the bison, both plains and wood bison, are more than just iconic symbols of the Canadian West.\n",
            "\n",
            "Feldolgozott (lemmatizált/normalizált) szöveg eleje:\n",
            "by natalie galan in the heart of alberta the bis both plains and wood bis are more than just iconic symbols of the canadian west\n"
          ]
        }
      ],
      "source": [
        "# --- Tisztítási lépések alkalmazása ---\n",
        "df_article_processed['cleaned_text'] = df_article_processed['raw_extracted_text'].apply(clean_special_chars_and_normalize)\n",
        "df_article_processed['normalized_text'] = df_article_processed['cleaned_text'].str.lower()\n",
        "\n",
        "# Lemmatizálás\n",
        "if nlp_hu:\n",
        "    print(\"Lemmatizálás folyamatban...\")\n",
        "    df_article_processed['lemmatized_text'] = lemmatize_texts_batch(df_article_processed['normalized_text'].tolist(), nlp_hu)\n",
        "    df_article_processed['final_processed_text'] = df_article_processed['lemmatized_text']\n",
        "    print(\"Lemmatizálás befejezve.\")\n",
        "else:\n",
        "    print(\"Figyelem: A spaCy modell nem érhető el, a lemmatizálás kimarad. A 'normalized_text' lesz a 'final_processed_text'.\")\n",
        "    df_article_processed['lemmatized_text'] = df_article_processed['normalized_text']\n",
        "    df_article_processed['final_processed_text'] = df_article_processed['normalized_text']\n",
        "\n",
        "print(\"Tisztított szöveg eleje:\")\n",
        "print(df_article_processed['cleaned_text'].iloc[0][:500])\n",
        "print(\"\\nFeldolgozott (lemmatizált/normalizált) szöveg eleje:\")\n",
        "print(df_article_processed['final_processed_text'].iloc[0][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wzpVTC6AVbRe",
      "metadata": {
        "id": "wzpVTC6AVbRe"
      },
      "source": [
        "## 3.5. Szövegkategorizálás (Téma szerint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "NNl2QzP0VbRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNl2QzP0VbRe",
        "outputId": "0cf5cd8f-9f88-498f-d542-0682da5c31a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 3.5. Szövegkategorizálás (Téma szerint) ---\n",
            "Kategorizáló modell (MoritzLaurer/mDeBERTa-v3-base-mnli-xnli) betöltése...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modell (MoritzLaurer/mDeBERTa-v3-base-mnli-xnli) sikeresen betöltve.\n",
            "Szöveg kategorizálása a következő címkékkel: ['Űrkutatás', 'Tudomány', 'Technológia', 'Egészségügy', 'Életmód', 'Biológia', 'Környezetvédelem', 'Élelmiszertudomány', 'Innováció', 'Kutatás', 'Gazdaság', 'Politika', 'Kultúra', 'Sport']\n",
            "A cikk valószínűsített témája (MoritzLaurer/mDeBERTa-v3-base-mnli-xnli modell alapján): Kultúra (Konfidencia: 0.30)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- 3.5. Szövegkategorizálás (Téma szerint) ---\")\n",
        "\n",
        "def categorize_text_zero_shot(text_to_categorize, candidate_labels, model_name=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", truncation=True):\n",
        "    classifier = None\n",
        "    selected_model = model_name\n",
        "    print(f\"Kategorizáló modell ({selected_model}) betöltése...\")\n",
        "    try:\n",
        "        classifier = pipeline(\"zero-shot-classification\", model=selected_model, device=0 if torch.cuda.is_available() else -1)\n",
        "        print(f\"Modell ({selected_model}) sikeresen betöltve.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Hiba történt a zero-shot klasszifikációs modell ({selected_model}) betöltése közben: {e}\")\n",
        "        alternative_model = \"joeddav/xlm-roberta-large-xnli\"\n",
        "        if selected_model != alternative_model:\n",
        "            print(f\"Próbálkozás egy alternatív modellel: {alternative_model}\")\n",
        "            try:\n",
        "                classifier = pipeline(\"zero-shot-classification\", model=alternative_model, device=0 if torch.cuda.is_available() else -1)\n",
        "                selected_model = alternative_model\n",
        "                print(f\"Alternatív modell ({selected_model}) sikeresen betöltve.\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Hiba történt az alternatív zero-shot klasszifikációs modell ({selected_model}) betöltése közben is: {e2}\")\n",
        "                return \"Kategorizálás sikertelen (modellhiba)\", 0.0, selected_model\n",
        "        else:\n",
        "             return \"Kategorizálás sikertelen (modellhiba)\", 0.0, selected_model\n",
        "\n",
        "    if not classifier:\n",
        "        return \"Kategorizálás sikertelen (nincs modell)\", 0.0, selected_model\n",
        "\n",
        "    print(f\"Szöveg kategorizálása a következő címkékkel: {candidate_labels}\")\n",
        "    try:\n",
        "        max_chars_for_classification = 3000 # Kb. első 500-600 szó\n",
        "        if len(text_to_categorize) > max_chars_for_classification:\n",
        "            text_input_for_classifier = text_to_categorize[:max_chars_for_classification]\n",
        "        else:\n",
        "            text_input_for_classifier = text_to_categorize\n",
        "\n",
        "        result = classifier(text_input_for_classifier, candidate_labels, multi_label=False, truncation=truncation)\n",
        "        predicted_label = result['labels'][0]\n",
        "        confidence_score = result['scores'][0]\n",
        "        # print(f\"Prediktált kategória ({selected_model} modellel): {predicted_label} (konfidencia: {confidence_score:.4f})\")\n",
        "        return predicted_label, confidence_score, selected_model\n",
        "    except Exception as e:\n",
        "        print(f\"Hiba történt a zero-shot klasszifikáció végrehajtása során ({selected_model}): {e}\")\n",
        "        return \"Kategorizálás sikertelen (futtatási hiba)\", 0.0, selected_model\n",
        "\n",
        "# Kategorizálandó szöveg (a 'cleaned_text' használata ajánlott)\n",
        "text_for_categorization = df_article_processed['cleaned_text'].iloc[0]\n",
        "\n",
        "# Témakörök / Kategóriák magyarul\n",
        "candidate_topics_hu = [\n",
        "    \"Űrkutatás\", \"Tudomány\", \"Technológia\", \"Egészségügy\", \"Életmód\",\n",
        "    \"Biológia\", \"Környezetvédelem\", \"Élelmiszertudomány\", \"Innováció\",\n",
        "    \"Kutatás\", \"Gazdaság\", \"Politika\", \"Kultúra\", \"Sport\"\n",
        "]\n",
        "\n",
        "if text_for_categorization and text_for_categorization.strip():\n",
        "    predicted_topic, topic_confidence, used_model = categorize_text_zero_shot(\n",
        "        text_for_categorization,\n",
        "        candidate_topics_hu\n",
        "    )\n",
        "    df_article_processed['predicted_topic'] = predicted_topic\n",
        "    df_article_processed['topic_confidence'] = topic_confidence\n",
        "    df_article_processed['categorization_model'] = used_model\n",
        "    print(f\"A cikk valószínűsített témája ({used_model} modell alapján): {predicted_topic} (Konfidencia: {topic_confidence:.2f})\")\n",
        "else:\n",
        "    print(\"Nincs szöveg a kategorizáláshoz.\")\n",
        "    df_article_processed['predicted_topic'] = \"N/A\"\n",
        "    df_article_processed['topic_confidence'] = 0.0\n",
        "    df_article_processed['categorization_model'] = \"N/A\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PBygyG68VbRf",
      "metadata": {
        "id": "PBygyG68VbRf"
      },
      "source": [
        "## 4. Vektor Reprezentáció (Beágyazások) Generálása"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "QOzv6R_SVbRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzv6R_SVbRg",
        "outputId": "7e61b74d-77e5-42c6-bde7-59ff03b0f3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 4. Vektor Reprezentáció Generálása ---\n",
            "Beágyazó modell betöltése: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
            "Beágyazó modell (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) sikeresen betöltve.\n",
            "Beágyazás generálása a feldolgozott szöveghez ('final_processed_text')...\n",
            "Generált beágyazás alakja: (384,)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- 4. Vektor Reprezentáció Generálása ---\")\n",
        "\n",
        "# Beágyazó modell betöltése\n",
        "sbert_model = None\n",
        "sbert_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "try:\n",
        "    print(f\"Beágyazó modell betöltése: {sbert_model_name}\")\n",
        "    sbert_model = SentenceTransformer(sbert_model_name)\n",
        "    print(f\"Beágyazó modell ({sbert_model_name}) sikeresen betöltve.\")\n",
        "except Exception as e:\n",
        "    print(f\"Hiba a '{sbert_model_name}' beágyazó modell betöltése közben: {e}\")\n",
        "\n",
        "# A 'final_processed_text' (lemmatizált vagy csak normalizált) használata a beágyazáshoz.\n",
        "text_for_embedding = df_article_processed['final_processed_text'].iloc[0]\n",
        "article_embedding_vector = None\n",
        "\n",
        "if sbert_model and text_for_embedding and text_for_embedding.strip():\n",
        "    print(f\"Beágyazás generálása a feldolgozott szöveghez ('final_processed_text')...\")\n",
        "    article_embedding_vector = get_average_embedding(text_for_embedding, sbert_model)\n",
        "    if article_embedding_vector is not None:\n",
        "        print(\"Generált beágyazás alakja:\", article_embedding_vector.shape)\n",
        "        # print(\"Beágyazás első 5 értéke:\", article_embedding_vector[:5])\n",
        "        df_article_processed['embedding_vector'] = [article_embedding_vector]\n",
        "    else:\n",
        "        print(\"Nem sikerült beágyazást generálni.\")\n",
        "        df_article_processed['embedding_vector'] = [None]\n",
        "else:\n",
        "    if not sbert_model:\n",
        "        print(\"A beágyazó modell nem töltődött be, a beágyazás generálása kimarad.\")\n",
        "    else:\n",
        "        print(\"Nincs feldolgozott szöveg a beágyazáshoz.\")\n",
        "    df_article_processed['embedding_vector'] = [None]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dXS_ZoI4VbRh",
      "metadata": {
        "id": "dXS_ZoI4VbRh"
      },
      "source": [
        "## 5. Eredmények Mentése és Megjelenítése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "BGkB9ZmTVbRi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "BGkB9ZmTVbRi",
        "outputId": "aa83648b-5d39-4904-f79a-3455c13e193a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Feldolgozás vége ---\n",
            "\n",
            "Végső DataFrame releváns oszlopokkal:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    print(\\\"Be\\u00e1gyaz\\u00e1svektor nem ker\\u00fclt gener\\u00e1l\\u00e1sra\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://www.aiwc.ca/blog/duckweed-a-superfood-from-the-wetlands/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_topic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Kult\\u00fara\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3003251254558563,\n        \"max\": 0.3003251254558563,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3003251254558563\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categorization_model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"by natalie galan in the heart of alberta the bis both plains and wood bis are more than just iconic symbols of the canadian west\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-32c3340c-d150-4ebf-bd12-ed1622a052ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>predicted_topic</th>\n",
              "      <th>topic_confidence</th>\n",
              "      <th>categorization_model</th>\n",
              "      <th>final_processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.aiwc.ca/blog/duckweed-a-superfood-from-the-wetlands/</td>\n",
              "      <td>Kultúra</td>\n",
              "      <td>0.300325</td>\n",
              "      <td>MoritzLaurer/mDeBERTa-v3-base-mnli-xnli</td>\n",
              "      <td>by natalie galan in the heart of alberta the bis both plains and wood bis ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32c3340c-d150-4ebf-bd12-ed1622a052ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32c3340c-d150-4ebf-bd12-ed1622a052ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32c3340c-d150-4ebf-bd12-ed1622a052ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                 id  \\\n",
              "0  https://www.aiwc.ca/blog/duckweed-a-superfood-from-the-wetlands/   \n",
              "\n",
              "  predicted_topic  topic_confidence                     categorization_model  \\\n",
              "0         Kultúra          0.300325  MoritzLaurer/mDeBERTa-v3-base-mnli-xnli   \n",
              "\n",
              "                                                              final_processed_text  \n",
              "0  by natalie galan in the heart of alberta the bis both plains and wood bis ar...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "A feldolgozott (lemmatizált/normalizált) szöveg elmentve ide: feldolgozott_szoveg.txt\n",
            "A nyers, kinyert szöveg elmentve ide: nyers_szoveg.txt\n",
            "A tisztított (de nem lemmatizált) szöveg elmentve ide: tisztitott_szoveg.txt\n",
            "\n",
            "Kategorizálás eredménye: Téma = Kultúra, Konfidencia = 0.30, Modell = MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
            "Beágyazásvektor generálva, alakja: (384,)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Feldolgozás vége ---\")\n",
        "\n",
        "# DataFrame megjelenítése (hasznos lehet notebookban)\n",
        "print(\"\\nVégső DataFrame releváns oszlopokkal:\")\n",
        "pd.set_option('display.max_colwidth', 80)\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(df_article_processed[['id', 'predicted_topic', 'topic_confidence', 'categorization_model', 'final_processed_text']])\n",
        "except ImportError:\n",
        "    print(df_article_processed[['id', 'predicted_topic', 'topic_confidence', 'categorization_model', 'final_processed_text']].head())\n",
        "\n",
        "\n",
        "output_filename_processed = \"feldolgozott_szoveg.txt\"\n",
        "with open(output_filename_processed, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df_article_processed['final_processed_text'].iloc[0])\n",
        "print(f\"\\nA feldolgozott (lemmatizált/normalizált) szöveg elmentve ide: {output_filename_processed}\")\n",
        "\n",
        "output_filename_raw = \"nyers_szoveg.txt\"\n",
        "with open(output_filename_raw, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df_article_processed['raw_extracted_text'].iloc[0])\n",
        "print(f\"A nyers, kinyert szöveg elmentve ide: {output_filename_raw}\")\n",
        "\n",
        "output_filename_cleaned = \"tisztitott_szoveg.txt\"\n",
        "with open(output_filename_cleaned, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df_article_processed['cleaned_text'].iloc[0])\n",
        "print(f\"A tisztított (de nem lemmatizált) szöveg elmentve ide: {output_filename_cleaned}\")\n",
        "\n",
        "print(f\"\\nKategorizálás eredménye: Téma = {df_article_processed['predicted_topic'].iloc[0]}, Konfidencia = {df_article_processed['topic_confidence'].iloc[0]:.2f}, Modell = {df_article_processed['categorization_model'].iloc[0]}\")\n",
        "if df_article_processed['embedding_vector'].iloc[0] is not None:\n",
        "    print(f\"Beágyazásvektor generálva, alakja: {df_article_processed['embedding_vector'].iloc[0].shape}\")\n",
        "else:\n",
        "    print(\"Beágyazásvektor nem került generálásra.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
